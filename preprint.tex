\section{Introduction}

Dependent types have been met with considerable interest from the research community in recent years.
Their primary application so far has been in proof assistants such as Coq~\citep{bertot2004interactive} and Agda~\citep{norell2007towards}, where they provide a sound and expressive foundation for theorem proving.
However, dependent types are still largely absent from general-purpose programming languages, despite a long history of lightweight approaches~\citep{xi1998eliminating}.
In the context of Haskell, much research has gone into extending the language to support computations on types, for instance in the form of functional dependencies~\citep{jones2000type}, type families~\citep{kiselyov2010fun} and promoted datatypes~\citep{yorgey2012giving}.
These techniques have seen vivid adoption by Haskell programmers, showing that there is a real demand for such mechanisms.
Furthermore, recent research has explored how dependent types could be added to the language for the same purpose~\citep{eisenberg2016dependent, weirich2017a}.

Dependently-typed languages often rely on a unified syntax to describe both terms and types.
The simplicity of this approach is unfortunately at odds with the design of most programming languages, where types and terms are expressed using separate syntactic categories.
Singleton types provide a simple solution to this problem by allowing terms to be represented as types.

In this chapter, we report on our attempt to generalize Scala's singleton types to support type-level programming, as well as a lightweight form of dependently-typed programming.
Unlike proof assistants, we do not aim to use types as a general-purpose logic, which would favor designs ensuring the totality of functions through termination checks.
Instead, our focus is on improving type safety by increasing the expressive power of the type system.

With these goals in mind, we extended Scala's type system to lift programs to the type level and partially evaluate them as part of type-checking.
Users can manipulate those types either directly, using new syntactic forms in Scala's type language, or have them inferred automatically using a new language keyword.
This effectively allows users to execute programs involving functions and pattern matching at the type level.

The remaining of the chapter is organized as follows:

\begin{itemize}
\item
  We begin by motivating why type-level programming is desirable and how one might use our Scala extension to improve type safety (\Cref{sec:motivating-example}). Our example demonstrates how to design a strongly-typed API in a functional style accessible to programmers.
\item
  We describe how we extended Scala with a generalization of singleton types (\Cref{sec:extending-scala}).
  We prototyped our type system on top of Dotty, the reference Scala~3 compiler.
  This practical introduction would be of interest to any Scala programmer willing to learn how to use our system, as well as programming language designers interested in dependent types.
\item
  We show a concrete use-case of our system by implementing a strongly-typed wrapper for Apache Spark~\citep{zaharia2016apache} (\Cref{sec:use-case}).
  Thanks to our generalized singleton types, we can statically ensure the type safety of database operations such as join and filter.
  We compare our implementation with an equivalent implicit-based one and show remarkable compilation time savings.
\end{itemize}

Our original presentation of generalized singleton types contains a formalization \citep[Section 3 and 4]{schmid2020coming}, which is out of the scope of this chapter.

\section{Motivating Example}
\label{sec:motivating-example}

We begin by motivating why type-level programming is desirable in general purpose programming.
In our first example, we design an API that keeps track of database tables' schemas in the type, and uses that information to improve type safety.
The examples in this section are written in our Scala extension described in \Cref{sec:extending-scala}.

As a first step, we show how our system supports type-level programming in the style of term-level programs.
Consider the following definition of the list datatype, which is standard Scala up the |dependent| keyword:

\begin{lstlisting}
sealed trait Lst { (*\text{\dots}*) }
dependent case class Cons(head: Any, tail: Lst) extends Lst
dependent case class Nil() extends Lst
\end{lstlisting}

\noindent
We can define list concatenation in the usual functional style of Scala, that is, using pattern matching and recursion:

\begin{lstlisting}
sealed trait Lst:
  dependent def concat(that: Lst) <: Lst =
    this match
      case Cons(x, xs) => Cons(x, concat(xs, that))
      case Nil() => that
\end{lstlisting}

\noindent
By annotating a method as |dependent|, the user instructs our system that the result type of |concat| should be as precise as its implementation.
Effectively, this means that the body of |concat| is lifted to the type level in a singleton type, and will be partially evaluated at every call site to compute a precise result type which \emph{depends} on the given inputs.
For recursive |dependent| methods such as |concat|, we infer types that include calls to |concat| itself.
The |<:| annotation lets us provide an upper bound on |concat|'s result type, which will be used while type checking the method's definition.
Finally, by qualifying the definition of |Cons| and |Nil| as |dependent|, we also allow their constructors and extractors to be lifted to the type level.
Using these definitions, we can now request the precise type whenever we manipulate lists by annotating |val| bindings as |dependent|:

\begin{lstlisting}
dependent val l1 = Cons("A", Nil())
dependent val l2 = Cons("B", Nil())
dependent val l3 = l1.concat(l2)
l3.size: { 2 }
l3: { Cons("A", Cons("B", Nil())) }
\end{lstlisting}

Enclosing a pure term in braces (|{|~\ldots{}~|}|) denotes the singleton type of that term.
In the last two lines of this example, we are therefore asking our system to prove that |l3| has size 2 and is equivalent to |Cons("A", Cons("B", Nil()))|.
Similarly, we can define remove on |Lst|:

\begin{lstlisting}
sealed trait Lst:
  dependent def remove(e: String) <: Lst =
    this match
      case Cons(head, tail) =>
        if (e == head) tail
        else Cons(head, tail.remove(e))
      case _ => throw new Error("element not found")
\end{lstlisting}

\noindent
Removing |"B"| yields the expected result, while trying to remove |"C"| from |l3| leads to a \emph{compilation error}, since the given program will provably fail at runtime.

\begin{lstlisting}
l3.remove("B"): { Cons("A", Nil()) }
l3.remove("C") // Error: element not found
\end{lstlisting}

The lists we defined so far can be used to implement a type-safe interface for database tables:

\begin{lstlisting}
dependent case class Table(schema: Lst, data: spark.DataFrame):
  dependent def join(right: Table, col: String) <: Table =
    val s1 = this.schema.remove(col)
    val s2 = right.schema.remove(col)
    val newSchema = Cons(col, s1.concat(s2))
    val newData = this.data.join(right.data, col)
    new Table(newSchema, newData)
\end{lstlisting}

\noindent
In this example, we wrap a Spark's |DataFrame| in the |dependent| class |Table|.
The first argument of this class represents the schema of the table as a precisely-typed list.
The second argument is the underlying |DataFrame|.
In the implementation of |join|, we execute the join operation on the underlying tables (|newData|) and compute the resulting schema corresponding to that join (|newSchema|).
By annotating the |join| method as |dependent|, the resulting schema is reflected in the type:

\begin{lstlisting}
dependent val schema1 = Cons("age", Cons("name", Nil()))
dependent val schema2 = Cons("name", Cons("unit", Nil()))
dependent val table1  = Table(schema1, (*\text{\dots}*))
dependent val table2  = Table(schema2, (*\text{\dots}*))
dependent val joined  = table1.join(table2, "name")
joined: { Table(Cons("name", Cons("age", Cons("unit", Nil()))), _: DataFrame) }
\end{lstlisting}

\noindent
Reflecting table schemas in types increases type safety over the existing weakly-typed interface.
For instance, it becomes possible to raise compile-time errors when a user tries to use non-existent columns.
This is an improvement over the underlying Spark implementation that would instead fail at runtime.

\section{Implementation}
\label{sec:extending-scala}

In this section we give an overview of how we extended Scala with a generalized notion of singleton types, and how we manipulate these types during type-checking.
Our presentation is divided into three parts.

First, we introduce new types that reflect a subset of Scala's term language at the type level, and give an informal description of our type evaluation algorithm.
We then introduce the |dependent| qualifier, which influences type inference to assign generalized singleton types to the annotated definition.
Finally, we discuss how our extension interacts with other aspects of Scala, such as side effects, virtual dispatch, and recursion.

This development was an experiment to explore the feasibility of adding a lightweight form of dependent types to Scala.
We implemented our prototype as an extension of Dotty, the reference Scala~3 compiler.

\subsection{Reflecting Terms in Types}

The fundamental difference between our and Scala's existing type system is that we take Scala's notion of singleton types, that is, precise types for variable bindings and literals, and extend them to cover Scala's core functional expressions.
Concretely we add new types for the following constructs:

\begin{itemize}
  \item Variable bindings and member selections
  \item Primitive literals
  \item Method calls
  \item If-then-else expressions
  \item Pattern matching expressions
  \item Constructor calls
\end{itemize}

These new types can be expressed using the |{| $e$ |}| type syntax, where $e$ is an expression in the core functional subset of Scala listed above.
For instance, suppose $\text{foo}$ stands for the expression

\begin{lstlisting}
if (x % 2 == 0) "even" else "odd"
\end{lstlisting}

\noindent then $\text{foo}$ can be typed as |{| $\text{foo}$ |}|.
More precisely, |{| $\text{foo}$ |}| is the singleton type of $\text{foo}$ and corresponds to the unique value of $\text{foo}$ in a given context.

The first two constructs in this list have antecedents in Scala.
Types for bindings are available since the early days of Scala with the |x.type| syntax \citep[Section 3.2.1]{odersky2006scala}, which is equivalent to |{ x }| in our system.
Types for primitive literals have recently been added to the language: literals for booleans, strings and the various numeric types are made available in the type language \citep{leontiev2014sip}.

In addition to base types and singleton types for every pure term, our system also supports types that lie in-between. We add a new form, |_: T|, which supplements expressions in types of form |{| $e$ |}| by allowing the user to intersperse terms and base types.

For instance, consider the following list and its singleton type:

\begin{itemize}
  \item |Cons(Nil, Nil): { Cons(Nil, Nil) }|
\end{itemize}

In our system, this term may also be typed less precisely, such as:

\begin{itemize}
  \item |Cons(Nil, Nil): { Cons(_: Any, Nil) }|
  \item |Cons(Nil, Nil): { Cons(Nil, _: Lst) }|
  \item |Cons(Nil, Nil): { Cons(_: Any, _: Lst) }|
\end{itemize}

\noindent
all of which are subsumed by

\begin{itemize}
  \item |Cons(Nil, Nil): Lst|.
\end{itemize}

\subsection{Type Evaluation}

Our system evaluates types using a call-by-value partial evaluator, which we embedded in the type-checker.
The evaluation rules for the term in generalized singleton types are completely standard up to dynamic type tests.

We perform evaluation of types during subtyping.
For instance, consider the following definition of a |parity| function:

\begin{lstlisting}
dependent def parity(x: Int) =
  if (x % 2 == 0) "even" else "odd"
\end{lstlisting}

\noindent
In order to prove the well-typedness of

\begin{lstlisting}
val p: { "odd" } = parity(5)
\end{lstlisting}

\noindent
the type-checker will ensure that |{ parity(5) }| is a subtype of |{ "odd" }|.
In the process, the left-hand side is evaluated as follows:

\begin{lstlisting}
  { parity(5) }                          (*$(1)$*)
= { if (5 % 2 == 0) "even" else "odd" }  (*$(2)$*)
= { if (false) "even" else "odd" }       (*$(3)$*)
= { "odd" }
\end{lstlisting}

\noindent
In (1), the method call to |parity| is replaced by its result type, where the method parameter has been substituted by the singleton type of the argument, |{ 5 }|.
(2) evaluates the boolean expression to |false|.
(3) reduces the |if| expression to its else branch.

In general, our evaluator will execute operations on concrete primitive values of types such as |Boolean|, |Int| and |String|, i.e., perform constant-folding.

\subsection{Pattern Matching}
\label{pattern-matching}

Pattern matching in Scala supports a wide range of matching techniques~\citep{emir2007matching}.
For example, \emph{extractor patterns} rely on user-defined methods to extract values from objects.
As a result, these custom extractors can contain arbitrary side effects.
Our implementation limits the kind of patterns available in types to the two simplest forms: decomposition of case classes and the type-tests/type-casts patterns.

During type normalization, our system evaluates pattern matching expressions according to Scala's runtime semantics.
Patterns are checked top-to-bottom, and type-tests are evaluated using runtime type information available after type erasure.

For example, consider the following pattern matching expression:

\begin{lstlisting}
s match {
  case _: T1 => v1
  case _: T2 => v2
}
\end{lstlisting}

\noindent
When used in a type, this expression reduces to |v1| if the scrutinee's type is a subtype of |T1|.
In order to reduce to |v2|, type normalization must make sure |T1| and the scrutinee's type are disjoint, namely that the dynamic type of |s| cannot possibly be smaller than |T1|.
Disjointness proofs are built using static knowledge about the class hierarchy, such as the |sealed| and |final| qualifiers, which are Scala's way of declaring closed-type hierarchies.

\subsection{Two Modes of Type Inference}

In order to retain backwards-compatibility, our system supports two modes of type inference: the precise inference mode which infers singleton types, and the default inference mode that corresponds to Scala's type-inference algorithm.
Concretely, users opt into our new inference mode using the |dependent| qualifier on methods, values, and classes.

When inferring the result type of a |dependent| method, our system lifts the method's body into a generalized singleton type.
This lifting will be precise for the subset of expressions that is representable in types, and approximative for the rest.
When we encounter an unsupported construct, we compute its type using the default mode, yielding a type |T| which we then integrate in the lifted body as |_: T|.

For example, given the following definition:

\begin{lstlisting}
dependent def getName(personalized: Boolean) =
  if (personalized) readString() else "Joe"
\end{lstlisting}

\noindent
our system infers the following result type:

\begin{lstlisting}
{ if (personalized) (_: String) else "Joe" }
\end{lstlisting}

\noindent
We could have equivalently defined |getName| by omitting the |dependent| qualifier and writing its result type  explicitly.

Scala requires recursive methods to have an explicit result type, and this restriction also applies to |dependent| methods.
However, in the case of a |dependent| method, an explicit result type is only used as an upper bound for the actual precise result type and will only be used to type-check the method's body.
At other call sites, the (precise) inferred result type is used.
Bounds of dependent methods are written using a special syntax (|<: T|).

\subsection{Approximating Side Effects}

Scala's type system permits uncontrolled side effects in programs.
Given the absence of an effect system, result types of methods do not convey any information about the potential use of side effects in the method body.
The situation is analogous for |dependent| methods.
Since we uniformly approximate all side effects, we avoid the situation where a type refers to a value that may be modified during the program execution.
For instance, if |z| is a mutable integer variable, we will never introduce |z| in a singleton type.
However, we can still assign singleton types to expressions containing |z|, for example, we can type |Cons(z, Nil())| as |{ Cons(_: Int, Nil()) }|.

Similarly to how we model other side effects, exceptions are approximated in types.
Our type-inference algorithm uses a new error type, |Error(e)|, which we infer when raising an exception with |throw e|.
Exception handlers are typed imprecisely using the default mode of type-inference.
Exceptions thrown in statement positions are not reflected in singleton types, since the type of |{e1; e2}| is simply |{ e2 }|.
However, exceptions thrown in tail positions (such as in remove from \Cref{sec:motivating-example}) can lead to types normalizing to |Error(e)|.
In these cases, our type system can prove that the program execution will encounter exceptional behavior, and report a compilation error.
This approach is conservative in that it might reject programs that recover from exceptions.
Also note that this is a sanity check, rather than a guarantee of no exceptions occurring at runtime.
That is, depending on which rules are used during subtyping, it is possible to succeed without entering type normalization, resulting in such errors going undetected.
Despite these shortcomings, our treatment of exceptions results in a practical way to raise compile-time errors.
It would be interesting to explore the addition of an effect system to our Scala extension.

\subsection{Virtual Dispatch}

Our extension does not model virtual dispatch explicitly in singleton types.
Instead, the result type of a method call |t.m(|\ldots{}|)| is always the result type of |m| in |t|'s static type.
Consequently, |dependent| methods effectively become |final|, given that only a provably-equivalent implementation could be used to override it.

Special care must be taken when an imprecisely-typed method is overridden with a dependent one.
In this situation, the result type of a method invocation can lose precision depending on type of the receiver.
Calls to the |equals| methods are a common example of this: |equals| is defined at the top of Scala's type hierarchy as referential equality and can be overridden arbitrarily.
Given a class Foo with a |dependent| overrides of |equals|, calls to |Foo.equals(Any)| and |Any.equals(Foo)| are not equivalent; the former precisely reflects the equality defined in |Foo| whereas the latter merely returns a |Boolean|.

\subsection{Termination}

We distinguish two important aspects of termination: the termination of type-checked programs and the termination of our type checker.

Proving termination or totality of programs is a non-goal of our system.
Unlike proof assistants, Scala programs do not manipulate proof terms.
Consequently, the lack of totality checks does not affect Scala's present notion of safety.
Exceptions or infinite loops in the evaluation of a `dependent` method would prevent the completion of type-checking.

The second question is termination of our type checker.
Non-termination of type checking implies that the type checker can give three possible answers, "type correct", "type incorrect" or "do not know" (when type checking times out).
Treating "do not know" as "type incorrect" makes the non-termination unproblematic from a soundness perspective.
A similar argument is made for other dependently-typed languages with unbounded recursion, such as Dependent Haskell~\citep{eisenberg2016dependent} or Cayenne~\citep{augustsson1998cayenne}.
In practice, our system deals with infinite loops using a fuel mechanism.
Every evaluation step consumes a unit of fuel, and an error is reported when the compiler runs out of fuel.
The default fuel limit can be increased via a compiler flag to enable arbitrarily long compilation times.

\section{Use Case}
\label{sec:use-case}

In this section, we extend the motivating example presented in \Cref{sec:motivating-example} by building a type-safe interface for Spark datasets.
We use our Scala extension to implement a simple domain-specific type checker for the SQL-like expressions used in Spark.

\subsection{A Type-Safe Database Interface}

The type-safe interface presented in this section illustrates the expressive power of our system and is implemented purely as a library.
For brevity, our presentation only covers a small part of Spark's dataset interface, but the approach can be scaled to cover that interface in its entirety.
The type safety of database queries is a canonical example and has been studied in many different settings~\citep{leijen1999domain, kazerounian2019type, meijer2006linq, chlipala2010ur}.

The example built in \Cref{sec:motivating-example} uses lists of column names to represent schemas.
A straightforward improvement is to also track the type of columns as part of the schema.
Instead of using column names directly, we introduce the following |Column| class with a phantom type parameter |T| for the column type, and a field |name| for the column name:

\begin{lstlisting}
dependent case class Column[T](name: String) { (*\text{\dots}*) }
\end{lstlisting}

Table schemas become lists of |Column|-s and thereby gain precision.
The definition of |join| given in \Cref{sec:motivating-example} can be adapted to this new schema encoding to prevent joining two tables that have columns with matching names but different types.

A large proportion of the weakly-typed Spark interface is dedicated to building expressions on table columns.
Such expressions can currently be built from strings, in a subset of SQL, or using a Scala DSL which is essentially untyped.

The lack of type safety for column expressions can be particularly dangerous when mixing columns of different types.
The pitfall is caused by Spark's inconsistency: depending on types of columns and operations involved, programs will either crash at runtime, or, more dangerously, data will be silently converted from one type to another.

By keeping track of column types it becomes possible to enforce the well-typedness of column expressions.
As an example, consider the following Spark program:

\begin{lstlisting}
table.filter(table.col("a") + table.col("b") === table.col("c"))
\end{lstlisting}

We would like our interface to enforce the following safety properties:

\begin{itemize}
\item
  Columns $a$, $b$ and $c$ are part of the schema of
  |table|.
\item
  Addition is well-defined on columns $a$ and $b$.
\item
  The result of adding columns $a$ and $b$ can be compared with
  column $c$.
\item
  The overall column expression yields a |Boolean|, which
  conforms to filter's argument type.
\end{itemize}

Automatic conversions during equality checks can be prevented by restricting column equality to expressions of the same type |T|:

\begin{lstlisting}
dependent case class Column[T](k: String):
  def ===(that: Column[T]): Column[Boolean] = Column(s"(${this.k} === ${that.k})")
\end{lstlisting}

Addition in Spark is defined between numeric types and characters.
The result type of an addition depends on the operand types.
For numeric types, Spark will pick the larger of the operand types according to the following ordering: |Double > Long > Int > Byte|.
The situation is quite surprising with characters as any addition involving a |Char| will result in a |Double|.

Dependent types can be used to precisely model these conversions.
We define a type function to compute the result type of additions:

\begin{lstlisting}
def addRes(a: Any, b: Any) =
  (a, b) match
    case (_: Char, _: Char | Byte | Int  | Long | Double) => _: Double
    case (_: Byte, _: Byte | Int  | Long | Double)        => b
    case (_: Int,  _: Int  | Long | Double)               => b
    case (_: Long, _: Long | Double)                      => b
    case (_: Double, _: Double)                           => _: Double
    case (_: Byte | Int | Long | Double, _)               => addRes(b, a)
    case _ => throw new Error("incompatible types in addition")
type AddRes[A, B] = { addRes(_: A, _: B) }
\end{lstlisting}

Also note the use of recursion in the second-to-last case, to avoid duplicating symmetric cases.
The |AddRes| type can be used to define a |Column| addition that accurately models Spark's runtime:

\begin{lstlisting}
dependent case class Column[T] private (k: String):
  dependent def +[U](that: Column[U]) <: Column[_] =
    Column[AddRes[T, U]](s"(${this.k} + ${that.k})")
\end{lstlisting}

Allowing programmers to construct |Column|-s from string literals would defeat the purpose of a type-safe interface.
Instead, programmers should extract columns from a |Table|'s schema.
For that purpose, we implement the |col| method on |Table| and annotate the |Column| constructor as private.

\begin{lstlisting}
dependent case class Table(schema: Lst, data: spark.DataFrame):
  dependent def col(name: String) <: Column[_] =
    dependent def find(key: String, list: Lst) <: Any =
      list match
        case Cons(head: Column[_], tail) =>
          if (head.k == key) head else find(key, tail)
        case _ => throw new Error("column not found in schema")
    find(name, schema)
  dependent def filter(predicate: Column[Boolean]) <: Table =
    new Table(this.schema, this.data.filter(predicate.k))
\end{lstlisting}

The |col| method is implemented using a nested dependent method to find the column corresponding to the given name.
Thanks to the dependent annotation, the type-checker is able to statically evaluate calls to |col|.
Assuming the table's schema contains a column |a| of type |Int| and columns |b| and |c| of type |Long|, the compiler will be able to infer types as follows:

\begin{lstlisting}
val pred =    table.col("a")    +    table.col("b")    ===   table.col("c")
// Infers: { Column[Int]("a") }   { Column[Long]("b") }   { Column[Long]("c") }
\end{lstlisting}

\noindent
Given our definitions of column addition and equality, the overall |pred| expression is typed as |Column[Boolean]|.
Thus, the dependently-typed interface presented in this section successfully enforces all the safety properties stated above.

\subsection{Comparison to an Existing Technique}

\dependentVsImplicitBenchmarks{Comparing the compilation times of two implementations of list concatenation and join, logarithmic scale.}

Programmers have managed to find clever encodings that circumvent the lack of first-class support for type-level programming in many languages.
These encodings can be very cumbersome, as they often entail poor error reporting and a negative impact on compilation times~\citep{mcbride2002faking},~\citep{kiselyov2004strongly}.
In Scala, implicits are the primary mechanism by which programmers implement type-level programming~\citep{odersky2018simplicitly}.

Frameless~\citep{blanvillain2016frameless} is a Scala library that implements a type-safe interface for Spark by making heavy use of implicits.
Most type-level computations in this library are performed on the heterogeneous lists provided by Shapeless~\citep{sabin2011shapeless}.

We compared the dependently-typed Spark interface presented in this section against the implicit-based implementation of Frameless.
To do so, we isolated the implicit-based implementation of the |join| operation on table schemas, and compared its compilation time against the dependently-typed version presented in this section.
To evaluate the scalability of both approaches we generated test cases with varying schema sizes and compiled each test case in isolation.
A similar comparison is done for list concatenation, which constitutes a building block of |join|.

\Cref{fig:dependentVsImplicitBenchmarks} shows that, in both benchmarks, the dependently-typed implementation compiles faster than the version with implicits, and compilation time scales better with the size of the input.

In the join benchmark, we see that the implicit-based implementation exceeds 30 seconds of compilation time around the 200 columns mark, and continues to grow quadratically.
This can be explained by the nature of implicit resolution, which might backtrack during its search.
The compilation time of the dependently-typed implementation grows linearly and stays below one second until the 350 columns mark.
We were able to observe similar trends in the concatenation benchmark.
These measurements were obtained by averaging 120 compilations on a warm compiler, and have been performed on an i7-7700K Processor running Oracle JVM 1.8.0 on Linux.

\section{Related Work}

As of today, Haskell is perhaps closest to becoming dependently-typed among the general-purpose programming languages used in industry.
Haskell's type families~\citep{kiselyov2010fun} provide a direct way to express type-level computations.
Other language extensions such as functional dependencies~\citep{jones2000type} and promoted datatypes~\citep{yorgey2012giving} are also moving Haskell towards dependent types.
Nevertheless, programming in Haskell remains significantly different from using full-spectrum dependently-typed languages.
A significant difference is that Haskell imposes a strict separation between terms and types.
As a result, writing dependently-typed programs in Haskell often involves code duplication between types and terms.
These redundancies can be somewhat avoided using the singletons package~\citep{eisenberg2012dependently}, which uses meta-programming to automatically generate types from datatypes and function definitions.

In the context of Haskell, Eisenberg's work on Dependent Haskell~\citep{eisenberg2016dependent} is closest to ours, in that it adds first-class support for dependent types to an established language, in a backwards-compatible way.
Dependent Haskell supports general recursion without termination checks, which makes it less suitable for theorem proving.
While we share similar goals, our work is differentiated by the contrasting paradigms of Scala and Haskell.
Like many object-oriented languages, Scala is primarily built around subtyping and does not restrict the use of side effects.
Furthermore, Eisenberg's system provides control over the relevance of values and type parameters.
In contrast, our system does not support any erasure annotations and simply follows Scala's canonical erasure strategy: types are systematically erased to JVM types, and terms are left untouched.
\citeauthor{weirich2017a} established a fully mechanized type safety proof for the core of Dependent Haskell~\citep{weirich2017a}.

Cayenne is a Haskell-like language with dependent types introduced in 1998 by Augustsson~\citep{augustsson1998cayenne}.
Like Dependent Haskell, it resembles our system in its treatment of termination, and differs by being a purely functional programming language.
Cayenne's treatment of erasure is similar to Scala's: types are systematically erased.
Augustsson proves that Cayenne's erasure is semantics-preserving, but does not provide any other metatheoretical results.

Adding dependent types to object-oriented languages is a remarkably under-explored area of research.
A notable exception is the recent work of~\cite{kazerounian2019type} on adding dependent types to Ruby.
Their goals are very much aligned with ours: using type-level programming to increase program safety.
Given the extremely dynamic nature of Ruby, it is unsurprising that their solution greatly differs from ours.
In their work, type checking happens entirely at runtime and has to be performed at every function invocation to account for possible changes in function definitions.
Safety is obtained by inserting dynamic checks, similarly to gradual typing.
